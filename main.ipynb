{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from clean import clean\n",
    "from augment import augment\n",
    "from trial import Trial\n",
    "\n",
    "################################################################################\n",
    "# Begin global state\n",
    "################################################################################\n",
    "\n",
    "facts = pd.read_csv(\"rst_transitions.tab\", sep=\"\\t\", quoting=3)\n",
    "\n",
    "# some categories of columns\n",
    "lex_feats = [\"Top2-Stack\", \"Top1Span\", \"First-Queue\"]\n",
    "categorical_features = ['Top12-StackXML', 'Stack-QueueSType', \"Stack\", \"genre\", \"Stack-QueueSameSent\",\n",
    "                        \"Top12-StackSameSent\",\n",
    "                        'Top12-StackSameSent', 'Stack-QueueXML', 'Top12-StackSType',\n",
    "                        \"Top12-StackDir\", \"Stack-QueueDir\", \"First-QueueEduFunc\", \"Top1SpanEduFunc\"]\n",
    "numeric_features = ['First-QueueDist-To-Begin', 'Top2-StackLength-EDU', 'Top1-StackLength-EDU'] #'Top1-StacknEDUs']\n",
    "scale_features = ['Top2-StackDist-To-End', 'First-Queue-Len']\n",
    "text_features = ['First-Queue', 'Top1Span', 'Top2-Stack']\n",
    "\n",
    "# clean and augment data\n",
    "data = facts.copy(deep=True)\n",
    "data = clean(data)\n",
    "data = augment(data)\n",
    "data = data.sample(frac=1, random_state=42)\n",
    "def split(data):\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in splitter.split(data, data[\"label\"]):\n",
    "        train = data.loc[train_idx]\n",
    "        test = data.loc[test_idx]\n",
    "    return train, test\n",
    "train, test = split(data)\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top2-Stack\n",
      "['1', '10', '12', '15', '17', '18', '19', '2', '20', '2006', '2010', '2011', '2012', '2014', '2015', '2016', '21', '23', '24', '25', '3', '30', '4', '5', '6', '7', '8', '9', 'A', 'After', 'Although', 'American', 'And', 'Andy', 'Andy Warhol', 'As', 'At', 'Australia', 'Because', 'Bobby', 'But', 'By', 'Cara', 'Church', 'Church of', 'City', 'Daniel', 'Do', \"Do n't\", 'Earth', 'English', 'Fillmore', 'First', 'For', 'For example', 'Fort', 'Fort Lee', 'From', 'Galois', 'He', 'He was', 'Her', 'His', 'Holt', 'How', 'However', 'I', 'I am', 'I could', 'I d', 'I did', 'I do', 'I had', 'I have', 'I m', 'I mean', 'I said', 'I say', 'I think', 'I ve', 'I was', 'If', 'If you', 'Image', 'In', 'In the', 'Internet', 'Is', 'It', 'It is', 'It s', 'It was', 'January', 'Jenna', 'L2', 'Lee', 'Marbles', 'March', 'Mohamed', 'Montalvo']\n",
      "Top1Span\n",
      "['1', '10', '12', '15', '17', '2', '20', '2010', '2011', '2012', '2014', '2015', '23', '24', '3', '30', '4', '5', '6', '7', '8', '9', 'A', 'After', 'Airport', 'All', 'American', 'An', 'And', 'Andy', 'Andy Warhol', 'As', 'As a', 'At', 'Athens', 'Bernoulli', 'Bobby', 'But', 'By', 'California', 'Cara', 'Church', 'Church of', 'City', 'Daniel', 'Do', \"Do n't\", 'East', 'English', 'Fillmore', 'First', 'For', 'For example', 'Fort', 'Fort Lee', 'From', 'From the', 'Galois', 'Goode', 'Gordon', 'Great', 'Greek', 'He', 'He is', 'He was', 'Her', 'His', 'Holt', 'How', 'However', 'I', 'I am', 'I could', 'I d', 'I do', 'I had', 'I have', 'I m', 'I saw', 'I think', 'I ve', 'I was', 'If', 'If you', 'In', 'In the', 'In this', 'International', 'Internet', 'Is', 'Island', 'It', 'It is', 'It s', 'It was', 'January', 'Jerome', 'John', 'July', 'Just']\n",
      "First-Queue\n",
      "['1', '11', '12', '15', '17', '18', '19', '2', '20', '2010', '2011', '2012', '2014', '2015', '23', '24', '25', '3', '30', '4', '5', '6', '7', '8', '9', 'A', 'After', 'All', 'Also', 'Although', 'American', 'And', 'As', 'As a', 'At', 'Bernoulli', 'Bobby', 'But', 'By', 'California', 'Cara', 'Career', 'Catholic', 'Church', 'City', 'Cleveland', 'College', 'Daniel', 'Do', \"Do n't\", 'Do you', 'During', 'Early', 'Early life', 'Fillmore', 'Finally', 'For', 'Fort', 'Fort Lee', 'Francisco', 'From', 'Galois', 'Get', 'Get in', 'Google', 'Gordon', 'Greek', 'He', 'He was', 'Her', 'Here', 'His', 'Holt', 'How', 'However', 'I', 'I am', 'I could', 'I did', 'I do', 'I had', 'I have', 'I m', 'I think', 'I ve', 'I was', 'I would', 'If', 'If you', 'Image', 'In', 'In addition', 'In the', 'Internet', 'Is', 'It', 'It is', 'It s', 'It was', 'January']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "                  transformer_weights=None,\n",
       "                  transformers=[('num',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('identity',\n",
       "                                                  FunctionTransformer(accept_sparse=False,\n",
       "                                                                      check_inverse=True,\n",
       "                                                                      func=<function <lambda> at 0x7f23fa54f170>,\n",
       "                                                                      inv_kw_args=None,\n",
       "                                                                      inverse_func=None,\n",
       "                                                                      kw_args=None,\n",
       "                                                                      pass_y='deprecated',\n",
       "                                                                      validate=None))],\n",
       "                                          verbose=Fals...\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('ordinal',\n",
       "                                                  OrdinalEncoder(categories='auto',\n",
       "                                                                 dtype=<class 'numpy.float64'>))],\n",
       "                                          verbose=False),\n",
       "                                 ['Top12-StackXML', 'Stack-QueueSType', 'Stack',\n",
       "                                  'genre', 'Stack-QueueSameSent',\n",
       "                                  'Top12-StackSameSent', 'Top12-StackSameSent',\n",
       "                                  'Stack-QueueXML', 'Top12-StackSType',\n",
       "                                  'Top12-StackDir', 'Stack-QueueDir',\n",
       "                                  'First-QueueEduFunc', 'Top1SpanEduFunc'])],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "# Feature engineering\n",
    "################################################################################\n",
    "# This is effectively a 'do nothing transformer', we may need it below\n",
    "identity_transformer = FunctionTransformer(lambda x: x)\n",
    "\n",
    "def make_transformer(data, categorical_encoding='ordinal', print_features=True):\n",
    "    assert categorical_encoding in ['ordinal', 'one_hot']\n",
    "    # Store a vocabulary per feature\n",
    "    vocabs = {}\n",
    "    lex_vectorizers = {}\n",
    "    for feat in lex_feats:\n",
    "        cvec = CountVectorizer(lowercase=False,\n",
    "                               ngram_range=(1, 2),\n",
    "                               # vocabulary=whitelist,   # You can work with your own whitelist\n",
    "                               max_features=1000,  # Or work with the top 1000 most frequent items, or...\n",
    "                               token_pattern=u\"(?u)\\\\b\\\\S+\\\\b\",  # Use these settings if you want to keep punctuation\n",
    "                               analyzer=\"word\")\n",
    "        cvec.fit(data[feat])\n",
    "        vocabs[feat] = cvec.get_feature_names()\n",
    "        lex_vectorizers[feat] = cvec\n",
    "\n",
    "        print(feat)\n",
    "        print(vocabs[feat][:100])\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('identity', identity_transformer)\n",
    "    ])\n",
    "\n",
    "    scale_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_steps = []\n",
    "    categorical_steps += [('onehot', OneHotEncoder(handle_unknown='ignore'))] if categorical_encoding == 'one_hot' else []\n",
    "    categorical_steps += [('ordinal', OrdinalEncoder())] if categorical_encoding == 'ordinal' else []\n",
    "    categorical_transformer = Pipeline(steps=categorical_steps)\n",
    "\n",
    "    text_transformer = ColumnTransformer(transformers=[\n",
    "        ('count', lex_vectorizers[\"First-Queue\"], \"First-Queue\"),\n",
    "        ('count2', lex_vectorizers[\"Top1Span\"], \"Top1Span\"),\n",
    "        ('count3', lex_vectorizers[\"First-Queue\"], \"Top2-Stack\"),\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('sca', scale_transformer, scale_features),\n",
    "            ('text', text_transformer, text_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "make_transformer(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Trials\n",
    "################################################################################\n",
    "\n",
    "# A trial is an object that conceptually means \"a model run with a featureset\"\n",
    "# You hand it a ColumnTransformer in its constructor, and in return, it will:\n",
    "# - evaluate on test for you\n",
    "# - store the model in trial.model\n",
    "# - store the preds in trial.preds\n",
    "# - store the transformer in trial.transformer\n",
    "# and more!\n",
    "\n",
    "class XGBTrial(Trial):\n",
    "    def __init__(self, transformer, use_test=False, **kwargs):\n",
    "        self.method = \"decision_function\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        eval_rows = test if use_test else train\n",
    "\n",
    "        X = transformer.fit_transform(train)\n",
    "        y = train[\"label\"]\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            nthread=-1\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # predict\n",
    "        X_eval = transformer.transform(eval_rows)\n",
    "        preds = model.predict(X_eval)\n",
    "\n",
    "        # hold on to refs in case we want them later\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.model = model\n",
    "        self.preds = preds\n",
    "        self.transformer = transformer\n",
    "\n",
    "        # populate score attributes\n",
    "        self._perf(eval_rows[\"label\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: run this file with -i (e.g.: `python -i main.py`) to drop into a Python REPL\n",
      "Train, test sizes:\n",
      "(22702, 33) (5676, 33)\n",
      "Top2-Stack\n",
      "['1', '10', '12', '15', '17', '18', '19', '2', '20', '2006', '2010', '2011', '2012', '2014', '2015', '2016', '21', '23', '24', '25', '3', '30', '4', '5', '6', '7', '8', '9', 'A', 'After', 'Although', 'American', 'And', 'Andy', 'Andy Warhol', 'As', 'At', 'Australia', 'Because', 'Bobby', 'But', 'By', 'Cara', 'Church', 'Church of', 'City', 'Daniel', 'Do', \"Do n't\", 'Earth', 'English', 'Fillmore', 'First', 'For', 'For example', 'Fort', 'Fort Lee', 'From', 'Galois', 'He', 'He was', 'Her', 'His', 'Holt', 'How', 'However', 'I', 'I am', 'I could', 'I d', 'I did', 'I do', 'I had', 'I have', 'I m', 'I mean', 'I said', 'I say', 'I think', 'I ve', 'I was', 'If', 'If you', 'Image', 'In', 'In the', 'Internet', 'Is', 'It', 'It is', 'It s', 'It was', 'January', 'Jenna', 'L2', 'Lee', 'Marbles', 'March', 'Mohamed', 'Montalvo']\n",
      "Top1Span\n",
      "['1', '10', '12', '15', '17', '2', '20', '2010', '2011', '2012', '2014', '2015', '23', '24', '3', '30', '4', '5', '6', '7', '8', '9', 'A', 'After', 'Airport', 'All', 'American', 'An', 'And', 'Andy', 'Andy Warhol', 'As', 'As a', 'At', 'Athens', 'Bernoulli', 'Bobby', 'But', 'By', 'California', 'Cara', 'Church', 'Church of', 'City', 'Daniel', 'Do', \"Do n't\", 'East', 'English', 'Fillmore', 'First', 'For', 'For example', 'Fort', 'Fort Lee', 'From', 'From the', 'Galois', 'Goode', 'Gordon', 'Great', 'Greek', 'He', 'He is', 'He was', 'Her', 'His', 'Holt', 'How', 'However', 'I', 'I am', 'I could', 'I d', 'I do', 'I had', 'I have', 'I m', 'I saw', 'I think', 'I ve', 'I was', 'If', 'If you', 'In', 'In the', 'In this', 'International', 'Internet', 'Is', 'Island', 'It', 'It is', 'It s', 'It was', 'January', 'Jerome', 'John', 'July', 'Just']\n",
      "First-Queue\n",
      "['1', '11', '12', '15', '17', '18', '19', '2', '20', '2010', '2011', '2012', '2014', '2015', '23', '24', '25', '3', '30', '4', '5', '6', '7', '8', '9', 'A', 'After', 'All', 'Also', 'Although', 'American', 'And', 'As', 'As a', 'At', 'Bernoulli', 'Bobby', 'But', 'By', 'California', 'Cara', 'Career', 'Catholic', 'Church', 'City', 'Cleveland', 'College', 'Daniel', 'Do', \"Do n't\", 'Do you', 'During', 'Early', 'Early life', 'Fillmore', 'Finally', 'For', 'Fort', 'Fort Lee', 'Francisco', 'From', 'Galois', 'Get', 'Get in', 'Google', 'Gordon', 'Greek', 'He', 'He was', 'Her', 'Here', 'His', 'Holt', 'How', 'However', 'I', 'I am', 'I could', 'I did', 'I do', 'I had', 'I have', 'I m', 'I think', 'I ve', 'I was', 'I would', 'If', 'If you', 'Image', 'In', 'In addition', 'In the', 'Internet', 'Is', 'It', 'It is', 'It s', 'It was', 'January']\n",
      "Top2-Stack\n",
      "['1', '10', '12', '15', '17', '18', '19', '2', '20', '2006', '2010', '2011', '2012', '2014', '2015', '2016', '21', '23', '24', '25', '3', '30', '4', '5', '6', '7', '8', '9', 'A', 'After', 'Although', 'American', 'And', 'Andy', 'Andy Warhol', 'As', 'At', 'Australia', 'Because', 'Bobby', 'But', 'By', 'Cara', 'Church', 'Church of', 'City', 'Daniel', 'Do', \"Do n't\", 'Earth', 'English', 'Fillmore', 'First', 'For', 'For example', 'Fort', 'Fort Lee', 'From', 'Galois', 'He', 'He was', 'Her', 'His', 'Holt', 'How', 'However', 'I', 'I am', 'I could', 'I d', 'I did', 'I do', 'I had', 'I have', 'I m', 'I mean', 'I said', 'I say', 'I think', 'I ve', 'I was', 'If', 'If you', 'Image', 'In', 'In the', 'Internet', 'Is', 'It', 'It is', 'It s', 'It was', 'January', 'Jenna', 'L2', 'Lee', 'Marbles', 'March', 'Mohamed', 'Montalvo']\n",
      "Top1Span\n",
      "['1', '10', '12', '15', '17', '2', '20', '2010', '2011', '2012', '2014', '2015', '23', '24', '3', '30', '4', '5', '6', '7', '8', '9', 'A', 'After', 'Airport', 'All', 'American', 'An', 'And', 'Andy', 'Andy Warhol', 'As', 'As a', 'At', 'Athens', 'Bernoulli', 'Bobby', 'But', 'By', 'California', 'Cara', 'Church', 'Church of', 'City', 'Daniel', 'Do', \"Do n't\", 'East', 'English', 'Fillmore', 'First', 'For', 'For example', 'Fort', 'Fort Lee', 'From', 'From the', 'Galois', 'Goode', 'Gordon', 'Great', 'Greek', 'He', 'He is', 'He was', 'Her', 'His', 'Holt', 'How', 'However', 'I', 'I am', 'I could', 'I d', 'I do', 'I had', 'I have', 'I m', 'I saw', 'I think', 'I ve', 'I was', 'If', 'If you', 'In', 'In the', 'In this', 'International', 'Internet', 'Is', 'Island', 'It', 'It is', 'It s', 'It was', 'January', 'Jerome', 'John', 'July', 'Just']\n",
      "First-Queue\n",
      "['1', '11', '12', '15', '17', '18', '19', '2', '20', '2010', '2011', '2012', '2014', '2015', '23', '24', '25', '3', '30', '4', '5', '6', '7', '8', '9', 'A', 'After', 'All', 'Also', 'Although', 'American', 'And', 'As', 'As a', 'At', 'Bernoulli', 'Bobby', 'But', 'By', 'California', 'Cara', 'Career', 'Catholic', 'Church', 'City', 'Cleveland', 'College', 'Daniel', 'Do', \"Do n't\", 'Do you', 'During', 'Early', 'Early life', 'Fillmore', 'Finally', 'For', 'Fort', 'Fort Lee', 'Francisco', 'From', 'Galois', 'Get', 'Get in', 'Google', 'Gordon', 'Greek', 'He', 'He was', 'Her', 'Here', 'His', 'Holt', 'How', 'However', 'I', 'I am', 'I could', 'I did', 'I do', 'I had', 'I have', 'I m', 'I think', 'I ve', 'I was', 'I would', 'If', 'If you', 'Image', 'In', 'In addition', 'In the', 'Internet', 'Is', 'It', 'It is', 'It s', 'It was', 'January']\n",
      "Beginning XGB fitting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def get_column_names_from_ColumnTransformer(column_transformer):\n",
    "    col_name = []\n",
    "    for transformer_in_columns in column_transformer.transformers_[\n",
    "                                  :-1]:  # the last transformer is ColumnTransformer's 'remainder'\n",
    "        raw_col_name = transformer_in_columns[2]\n",
    "        if isinstance(transformer_in_columns[1], Pipeline):\n",
    "            transformer = transformer_in_columns[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = transformer_in_columns[1]\n",
    "        try:\n",
    "            names = transformer.get_feature_names()\n",
    "        except AttributeError:  # if no 'get_feature_names' function, use raw column name\n",
    "            names = raw_col_name\n",
    "        if isinstance(names, np.ndarray):  # eg.\n",
    "            col_name += names.tolist()\n",
    "        elif isinstance(names, list):\n",
    "            col_name += names\n",
    "        elif isinstance(names, str):\n",
    "            col_name.append(names)\n",
    "    return col_name\n",
    "\n",
    "\n",
    "# Scratch area\n",
    "oh_transformer = make_transformer(train, one_hot=True)\n",
    "ord_transformer = make_transformer(train, ordinal=True)\n",
    "\n",
    "print(\"Beginning XGB fitting...\")\n",
    "xgb_trial = XGBTrial(ord_transformer, use_test=True)\n",
    "print(xgb_trial)\n",
    "print(\"Fitting done. Predicting...\")\n",
    "\n",
    "print(classification_report(test[\"label\"], xgb_trial.preds))\n",
    "\n",
    "names = get_column_names_from_ColumnTransformer(ord_transformer)\n",
    "print(names[:50])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
