{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22702\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First-Queue</th>\n",
       "      <th>First-QueueDist-To-Begin</th>\n",
       "      <th>First-QueueEduFunc</th>\n",
       "      <th>First-QueueFullFunc</th>\n",
       "      <th>First-QueueFullPos</th>\n",
       "      <th>Queue</th>\n",
       "      <th>SeqPredFirstSpan</th>\n",
       "      <th>SeqPredTop1Span</th>\n",
       "      <th>Stack</th>\n",
       "      <th>Stack-QueueDir</th>\n",
       "      <th>...</th>\n",
       "      <th>Top1SpanEduFunc</th>\n",
       "      <th>Top1SpanFullFunc</th>\n",
       "      <th>Top1SpanFullPos</th>\n",
       "      <th>Top2-Stack</th>\n",
       "      <th>Top2-StackDist-To-Begin</th>\n",
       "      <th>Top2-StackDist-To-End</th>\n",
       "      <th>Top2-StackLength-EDU</th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>First-Queue-Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19473</td>\n",
       "      <td># People used to go to banks with gold and rec...</td>\n",
       "      <td>103.0</td>\n",
       "      <td>root</td>\n",
       "      <td># nsubj root mark xcomp case obl case obl cc c...</td>\n",
       "      <td># NNS VBD TO VB TO NNS IN NN CC VB `` NN '' IN...</td>\n",
       "      <td>NonEmpty</td>\n",
       "      <td>elabo</td>\n",
       "      <td>attri</td>\n",
       "      <td>MoreElem</td>\n",
       "      <td>NONE_NONE</td>\n",
       "      <td>...</td>\n",
       "      <td>ccomp</td>\n",
       "      <td># nsubj ccomp punct #</td>\n",
       "      <td># PRP VBZ . #</td>\n",
       "      <td># the gov't says #</td>\n",
       "      <td>101</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>attribution-SN</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4942</td>\n",
       "      <td># were not only marked by her experience #</td>\n",
       "      <td>94.0</td>\n",
       "      <td>root</td>\n",
       "      <td># aux:pass advmod advmod root case nmod:poss o...</td>\n",
       "      <td># VBD RB RB VBN IN PRP$ NN #</td>\n",
       "      <td>NonEmpty</td>\n",
       "      <td>same</td>\n",
       "      <td>elabo</td>\n",
       "      <td>MoreElem</td>\n",
       "      <td>NONE</td>\n",
       "      <td>...</td>\n",
       "      <td>nsubj:pass</td>\n",
       "      <td># det nsubj:pass case nmod:poss amod nmod #</td>\n",
       "      <td># DT NNS IN PRP$ JJ NN #</td>\n",
       "      <td># In 1893 , Higuchi , her mother and her siste...</td>\n",
       "      <td>79</td>\n",
       "      <td>33.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>bio</td>\n",
       "      <td>Shift</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18876</td>\n",
       "      <td># CMV , #</td>\n",
       "      <td>60.0</td>\n",
       "      <td>root</td>\n",
       "      <td># root punct #</td>\n",
       "      <td># VB , #</td>\n",
       "      <td>NonEmpty</td>\n",
       "      <td>elabo</td>\n",
       "      <td>elabo</td>\n",
       "      <td>MoreElem</td>\n",
       "      <td>NONE_NONE</td>\n",
       "      <td>...</td>\n",
       "      <td>root</td>\n",
       "      <td># det nsubj cop det advmod root punct #</td>\n",
       "      <td># DT NN VBZ DT JJS CD . #</td>\n",
       "      <td># This is n’t difficult or a big deal . #</td>\n",
       "      <td>57</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>joint-NN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9894</td>\n",
       "      <td># My mother 's more particular -- #</td>\n",
       "      <td>42.0</td>\n",
       "      <td>root</td>\n",
       "      <td># nmod:poss nsubj cop advmod root punct #</td>\n",
       "      <td># PRP$ NN VBZ RBR JJ : #</td>\n",
       "      <td>NonEmpty</td>\n",
       "      <td>elabo</td>\n",
       "      <td>elabo</td>\n",
       "      <td>MoreElem</td>\n",
       "      <td>NONE_NONE</td>\n",
       "      <td>...</td>\n",
       "      <td>parataxis</td>\n",
       "      <td># punct nmod:poss nsubj aux parataxis obj punc...</td>\n",
       "      <td># `` PRP$ NN MD VB PRP , '' #</td>\n",
       "      <td># \" Why ? \" #</td>\n",
       "      <td>38</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fiction</td>\n",
       "      <td>Shift</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6330</td>\n",
       "      <td># Career #</td>\n",
       "      <td>27.0</td>\n",
       "      <td>root</td>\n",
       "      <td># root #</td>\n",
       "      <td># NN #</td>\n",
       "      <td>NonEmpty</td>\n",
       "      <td>elabo</td>\n",
       "      <td>elabo</td>\n",
       "      <td>MoreElem</td>\n",
       "      <td>NONE_NONE</td>\n",
       "      <td>...</td>\n",
       "      <td>dep</td>\n",
       "      <td># punct dep punct #</td>\n",
       "      <td># -LSB- CD -RSB- #</td>\n",
       "      <td># Although he had originally planned to attend...</td>\n",
       "      <td>22</td>\n",
       "      <td>92.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>bio</td>\n",
       "      <td>evidence-NS</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             First-Queue  \\\n",
       "19473  # People used to go to banks with gold and rec...   \n",
       "4942          # were not only marked by her experience #   \n",
       "18876                                          # CMV , #   \n",
       "9894                 # My mother 's more particular -- #   \n",
       "6330                                          # Career #   \n",
       "\n",
       "       First-QueueDist-To-Begin First-QueueEduFunc  \\\n",
       "19473                     103.0               root   \n",
       "4942                       94.0               root   \n",
       "18876                      60.0               root   \n",
       "9894                       42.0               root   \n",
       "6330                       27.0               root   \n",
       "\n",
       "                                     First-QueueFullFunc  \\\n",
       "19473  # nsubj root mark xcomp case obl case obl cc c...   \n",
       "4942   # aux:pass advmod advmod root case nmod:poss o...   \n",
       "18876                                     # root punct #   \n",
       "9894           # nmod:poss nsubj cop advmod root punct #   \n",
       "6330                                            # root #   \n",
       "\n",
       "                                      First-QueueFullPos     Queue  \\\n",
       "19473  # NNS VBD TO VB TO NNS IN NN CC VB `` NN '' IN...  NonEmpty   \n",
       "4942                        # VBD RB RB VBN IN PRP$ NN #  NonEmpty   \n",
       "18876                                           # VB , #  NonEmpty   \n",
       "9894                            # PRP$ NN VBZ RBR JJ : #  NonEmpty   \n",
       "6330                                              # NN #  NonEmpty   \n",
       "\n",
       "      SeqPredFirstSpan SeqPredTop1Span     Stack Stack-QueueDir  ...  \\\n",
       "19473            elabo           attri  MoreElem      NONE_NONE  ...   \n",
       "4942              same           elabo  MoreElem           NONE  ...   \n",
       "18876            elabo           elabo  MoreElem      NONE_NONE  ...   \n",
       "9894             elabo           elabo  MoreElem      NONE_NONE  ...   \n",
       "6330             elabo           elabo  MoreElem      NONE_NONE  ...   \n",
       "\n",
       "      Top1SpanEduFunc                                   Top1SpanFullFunc  \\\n",
       "19473           ccomp                              # nsubj ccomp punct #   \n",
       "4942       nsubj:pass        # det nsubj:pass case nmod:poss amod nmod #   \n",
       "18876            root            # det nsubj cop det advmod root punct #   \n",
       "9894        parataxis  # punct nmod:poss nsubj aux parataxis obj punc...   \n",
       "6330              dep                                # punct dep punct #   \n",
       "\n",
       "                     Top1SpanFullPos  \\\n",
       "19473                  # PRP VBZ . #   \n",
       "4942        # DT NNS IN PRP$ JJ NN #   \n",
       "18876      # DT NN VBZ DT JJS CD . #   \n",
       "9894   # `` PRP$ NN MD VB PRP , '' #   \n",
       "6330              # -LSB- CD -RSB- #   \n",
       "\n",
       "                                              Top2-Stack  \\\n",
       "19473                                 # the gov't says #   \n",
       "4942   # In 1893 , Higuchi , her mother and her siste...   \n",
       "18876          # This is n’t difficult or a big deal . #   \n",
       "9894                                       # \" Why ? \" #   \n",
       "6330   # Although he had originally planned to attend...   \n",
       "\n",
       "      Top2-StackDist-To-Begin Top2-StackDist-To-End  Top2-StackLength-EDU  \\\n",
       "19473                     101                  68.0                   1.0   \n",
       "4942                       79                  33.0                  13.0   \n",
       "18876                      57                  74.0                   1.0   \n",
       "9894                       38                  67.0                   1.0   \n",
       "6330                       22                  92.0                   4.0   \n",
       "\n",
       "         genre           label First-Queue-Len  \n",
       "19473   reddit  attribution-SN              71  \n",
       "4942       bio           Shift              42  \n",
       "18876   reddit        joint-NN               9  \n",
       "9894   fiction           Shift              35  \n",
       "6330       bio     evidence-NS              10  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd, numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from clean import clean\n",
    "from augment import augment\n",
    "from trial import Trial\n",
    "\n",
    "################################################################################\n",
    "# Begin global state\n",
    "################################################################################\n",
    "\n",
    "facts = pd.read_csv(\"rst_transitions.tab\", sep=\"\\t\", quoting=3)\n",
    "\n",
    "# some categories of columns\n",
    "lex_feats = [\"Top2-Stack\", \"Top1Span\", \"First-Queue\"]\n",
    "categorical_features = ['Top12-StackXML', 'Stack-QueueSType', \"Stack\", \"genre\", \"Stack-QueueSameSent\",\n",
    "                        \"Top12-StackSameSent\",\n",
    "                        'Top12-StackSameSent', 'Stack-QueueXML', 'Top12-StackSType',\n",
    "                        \"Top12-StackDir\", \"Stack-QueueDir\", \"First-QueueEduFunc\", \"Top1SpanEduFunc\"]\n",
    "numeric_features = ['First-QueueDist-To-Begin', 'Top2-StackLength-EDU', 'Top1-StackLength-EDU'] #'Top1-StacknEDUs']\n",
    "scale_features = ['Top2-StackDist-To-End', 'First-Queue-Len']\n",
    "text_features = ['First-Queue', 'Top1Span', 'Top2-Stack']\n",
    "\n",
    "def load_bc():\n",
    "    with open('bc3200.pickle', 'rb') as f:\n",
    "        bc3200 = pickle.load(f)   \n",
    "    # treat something like '10101111' as a binary integer encoding\n",
    "    s2n = lambda s: sum([int(s[-i]) * (1 << i) for i in range(len(s))])\n",
    "    \n",
    "    converted = [(k, s2n(v)) for k,v in bc3200.items()]\n",
    "    # sort by bc encoding\n",
    "    return dict(sorted(converted, key=lambda x:x[1]))\n",
    "bc_dict = load_bc()\n",
    "\n",
    "# clean and augment data\n",
    "data = facts.copy(deep=True)\n",
    "data = clean(data)\n",
    "data = augment(data)\n",
    "data = data.sample(frac=1, random_state=42)\n",
    "def split(data):\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in splitter.split(data, data[\"label\"]):\n",
    "        train = data.loc[train_idx]\n",
    "        test = data.loc[test_idx]\n",
    "    return train, test\n",
    "train, test = split(data)\n",
    "print(len(train))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_transformer = FunctionTransformer(lambda x: x)\n",
    "def bc_encode_toks(toks):\n",
    "    toks = toks.split(\" \")\n",
    "    return [str(bc_dict[x]) if x in bc_dict else -1 for x in toks]\n",
    "bc_transformer = FunctionTransformer(bc_encode_toks)\n",
    "\n",
    "def make_text_transformer(\n",
    "    data,\n",
    "    brown=False\n",
    "):\n",
    "    # Store a vocabulary per feature\n",
    "    vocabs = {}\n",
    "    transformers = []\n",
    "    \n",
    "    for feat in lex_feats:\n",
    "        cvec = CountVectorizer(\n",
    "            lowercase=False,\n",
    "            ngram_range=(1, 2),\n",
    "            # vocabulary=whitelist,   # You can work with your own whitelist\n",
    "            max_features=1000,  # Or work with the top 1000 most frequent items, or...\n",
    "            token_pattern=u\"(?u)\\\\b\\\\S+\\\\b\",  # Use these settings if you want to keep punctuation\n",
    "            analyzer=\"word\"\n",
    "        )\n",
    "        cvec.fit(data[feat])\n",
    "        vocabs[feat] = cvec.get_feature_names()\n",
    "        transformers.append(('count_' + feat, cvec, feat))\n",
    "        \n",
    "        if brown:\n",
    "            cvec = CountVectorizer(\n",
    "                lowercase=False,\n",
    "                ngram_range=(1,2),\n",
    "                max_features=10000,\n",
    "                token_pattern=u\"(?u)\\\\b\\\\S+\\\\b\",\n",
    "                analyzer=\"word\"\n",
    "            )\n",
    "\n",
    "            transformers.append((\"brown_\" + feat, cvec, feat))\n",
    "\n",
    "    return ColumnTransformer(transformers)\n",
    "\n",
    "def make_categorical_transformer(categorical_encoding='ordinal'):\n",
    "    assert categorical_encoding in ['ordinal', 'one_hot']\n",
    "    categorical_steps = []\n",
    "    categorical_steps += [('onehot', OneHotEncoder(handle_unknown='ignore'))] if categorical_encoding == 'one_hot' else []\n",
    "    categorical_steps += [('ordinal', OrdinalEncoder())] if categorical_encoding == 'ordinal' else []\n",
    "    return Pipeline(steps=categorical_steps)   \n",
    "\n",
    "def make_transformer(\n",
    "    data, \n",
    "    numeric_transformer=None,\n",
    "    scale_transformer=None,\n",
    "    text_transformer=None,\n",
    "    categorical_transformer=None,\n",
    "    extra_transformers=[]\n",
    "):\n",
    "    \n",
    "    if scale_transformer is None:\n",
    "        scale_transformer = StandardScaler()\n",
    "    if numeric_transformer is None:\n",
    "        numeric_transformer = identity_transformer\n",
    "    if text_transformer is None:\n",
    "        text_transformer = make_text_transformer(data)\n",
    "    if categorical_transformer is None:\n",
    "        categorical_transformer = make_categorical_transformer()\n",
    "\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('sca', scale_transformer, scale_features),\n",
    "        ('text', text_transformer, text_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    "    transformers += extra_transformers\n",
    "    return ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Trials\n",
    "################################################################################\n",
    "\n",
    "# A trial is an object that conceptually means \"a model run with a featureset\"\n",
    "# You hand it a ColumnTransformer in its constructor, and in return, it will:\n",
    "# - evaluate on test for you\n",
    "# - store the model in trial.model\n",
    "# - store the preds in trial.preds\n",
    "# - store the transformer in trial.transformer\n",
    "# and more!\n",
    "\n",
    "class XGBTrial(Trial):\n",
    "    def __init__(self, transformer, use_test=False, **kwargs):\n",
    "        self.method = \"decision_function\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        eval_rows = test if use_test else train\n",
    "\n",
    "        X = transformer.fit_transform(train)\n",
    "        y = train[\"label\"]\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            nthread=-1\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # predict\n",
    "        X_eval = transformer.transform(eval_rows)\n",
    "        preds = model.predict(X_eval)\n",
    "\n",
    "        # hold on to refs in case we want them later\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.model = model\n",
    "        self.preds = preds\n",
    "        self.transformer = transformer\n",
    "\n",
    "        # populate score attributes\n",
    "        self._perf(eval_rows[\"label\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning XGB fitting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBTrial:\n",
      "  accuracy: 0.6663143058491896\n",
      "  micro_recall: 0.6663143058491896\n",
      "  macro_recall: 0.2506690953703519\n",
      "  micro_f1: 0.6663143058491896\n",
      "  macro_f1: 0.2782958144415433\n",
      "\n",
      "Fitting done. Predicting...\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Shift       0.74      0.94      0.83      2861\n",
      "  antithesis-NS       0.38      0.11      0.17        28\n",
      "  antithesis-SN       0.33      0.04      0.07        27\n",
      " attribution-NS       0.79      0.67      0.72        33\n",
      " attribution-SN       0.68      0.67      0.68       147\n",
      "  background-NS       0.00      0.00      0.00        38\n",
      "  background-SN       0.00      0.00      0.00        73\n",
      "       cause-NS       0.73      0.19      0.30        42\n",
      "       cause-SN       0.00      0.00      0.00        33\n",
      "circumstance-NS       0.58      0.45      0.51        78\n",
      "circumstance-SN       0.67      0.19      0.29        64\n",
      "  concession-NS       0.62      0.10      0.18        49\n",
      "  concession-SN       0.47      0.18      0.26        44\n",
      "   condition-NS       0.60      0.40      0.48        15\n",
      "   condition-SN       0.79      0.63      0.70        35\n",
      "    contrast-NN       0.39      0.18      0.25        94\n",
      " elaboration-NS       0.52      0.51      0.52       557\n",
      " elaboration-SN       0.00      0.00      0.00         0\n",
      "  evaluation-NS       0.33      0.01      0.02       104\n",
      "  evaluation-SN       0.00      0.00      0.00         5\n",
      "    evidence-NS       0.81      0.55      0.65        93\n",
      "    evidence-SN       0.00      0.00      0.00         8\n",
      "       joint-NN       0.42      0.55      0.48       379\n",
      "     justify-NS       0.33      0.02      0.04        43\n",
      "     justify-SN       0.00      0.00      0.00         8\n",
      "      manner-NS       1.00      0.03      0.06        30\n",
      "       means-NS       0.60      0.64      0.62        14\n",
      "       means-SN       1.00      1.00      1.00         1\n",
      "  motivation-NS       0.33      0.10      0.15        20\n",
      "  motivation-SN       0.00      0.00      0.00        13\n",
      " preparation-SN       0.63      0.47      0.54       152\n",
      "     purpose-NS       0.52      0.59      0.55        80\n",
      "     purpose-SN       0.25      0.17      0.20         6\n",
      "    question-SN       0.75      0.39      0.51        31\n",
      " restatement-NN       0.00      0.00      0.00        21\n",
      " restatement-NS       0.67      0.14      0.24        28\n",
      " restatement-SN       0.00      0.00      0.00         1\n",
      "      result-NS       0.00      0.00      0.00        50\n",
      "      result-SN       0.00      0.00      0.00         1\n",
      "   same_unit-NN       0.51      0.52      0.52       149\n",
      "    sequence-NN       0.62      0.34      0.44       210\n",
      "solutionhood-NS       0.00      0.00      0.00         1\n",
      "solutionhood-SN       0.00      0.00      0.00        10\n",
      "\n",
      "       accuracy                           0.67      5676\n",
      "      macro avg       0.40      0.25      0.28      5676\n",
      "   weighted avg       0.62      0.67      0.62      5676\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def get_column_names_from_ColumnTransformer(column_transformer):\n",
    "    col_name = []\n",
    "    for transformer_in_columns in column_transformer.transformers_[\n",
    "                                  :-1]:  # the last transformer is ColumnTransformer's 'remainder'\n",
    "        raw_col_name = transformer_in_columns[2]\n",
    "        if isinstance(transformer_in_columns[1], Pipeline):\n",
    "            transformer = transformer_in_columns[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = transformer_in_columns[1]\n",
    "        try:\n",
    "            names = transformer.get_feature_names()\n",
    "        except AttributeError:  # if no 'get_feature_names' function, use raw column name\n",
    "            names = raw_col_name\n",
    "        if isinstance(names, np.ndarray):  # eg.\n",
    "            col_name += names.tolist()\n",
    "        elif isinstance(names, list):\n",
    "            col_name += names\n",
    "        elif isinstance(names, str):\n",
    "            col_name.append(names)\n",
    "    return col_name\n",
    "\n",
    "\n",
    "# Default settings\n",
    "oh_transformer = make_transformer(train)\n",
    "ord_transformer = make_transformer(train)\n",
    "\n",
    "print(\"Beginning XGB fitting...\")\n",
    "xgb_trial = XGBTrial(ord_transformer, use_test=True)\n",
    "print(xgb_trial)\n",
    "print(\"Fitting done. Predicting...\")\n",
    "#print(classification_report(test[\"label\"], xgb_trial.preds))\n",
    "#names = get_column_names_from_ColumnTransformer(ord_transformer)\n",
    "#print(names[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/luke/.anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "bc_transformer = make_transformer(\n",
    "    train,\n",
    "    text_transformer=make_text_transformer(train, brown=True)\n",
    ")\n",
    "xgb_with_bc = XGBTrial(bc_transformer, use_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBTrial:\n",
      "  accuracy: 0.6663143058491896\n",
      "  micro_recall: 0.6663143058491896\n",
      "  macro_recall: 0.2506690953703519\n",
      "  micro_f1: 0.6663143058491896\n",
      "  macro_f1: 0.2782958144415433\n",
      "\n",
      "XGBTrial:\n",
      "  accuracy: 0.6664904862579282\n",
      "  micro_recall: 0.6664904862579282\n",
      "  macro_recall: 0.2275469805498872\n",
      "  micro_f1: 0.6664904862579282\n",
      "  macro_f1: 0.2561203143017332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(xgb_trial, name='standard')\n",
    "print(xgb_with_bc, name='standard with brown cluster')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
